name: Performance Benchmarks

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for comparisons

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[benchmark]"

      - name: Download baseline benchmarks
        id: download-baseline
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: benchmark-baseline
          path: .benchmarks/

      - name: Run benchmarks
        run: |
          pytest tests/test_performance.py --benchmark-only --benchmark-json=benchmark-results.json --benchmark-save=current

      - name: Compare against baseline
        id: compare
        if: steps.download-baseline.outcome == 'success'
        continue-on-error: true
        run: |
          # Compare current results with baseline
          pytest tests/test_performance.py --benchmark-only --benchmark-compare=baseline --benchmark-compare-fail=mean:10%

      - name: Generate benchmark report
        if: always()
        run: |
          echo "# Performance Benchmark Results" > benchmark-report.md
          echo "" >> benchmark-report.md
          echo "**Run Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')" >> benchmark-report.md
          echo "**Python Version:** $(python --version)" >> benchmark-report.md
          echo "" >> benchmark-report.md

          # Parse benchmark results
          python << 'EOF'
          import json
          import sys
          from pathlib import Path

          try:
              with open('benchmark-results.json') as f:
                  data = json.load(f)

              with open('benchmark-report.md', 'a') as report:
                  report.write("## Benchmark Summary\n\n")
                  report.write("| Test | Min (ms) | Max (ms) | Mean (ms) | StdDev (ms) |\n")
                  report.write("|------|----------|----------|-----------|-------------|\n")

                  benchmarks = data.get('benchmarks', [])
                  for bench in benchmarks:
                      name = bench.get('name', 'Unknown')
                      stats = bench.get('stats', {})
                      min_time = stats.get('min', 0) * 1000
                      max_time = stats.get('max', 0) * 1000
                      mean_time = stats.get('mean', 0) * 1000
                      stddev = stats.get('stddev', 0) * 1000

                      report.write(f"| {name} | {min_time:.3f} | {max_time:.3f} | {mean_time:.3f} | {stddev:.3f} |\n")

                  # Check for regressions
                  if Path('.benchmarks/baseline.json').exists():
                      with open('.benchmarks/baseline.json') as f:
                          baseline = json.load(f)

                      report.write("\n## Regression Analysis\n\n")
                      baseline_benchmarks = {b['name']: b for b in baseline.get('benchmarks', [])}

                      regressions = []
                      for bench in benchmarks:
                          name = bench.get('name')
                          if name in baseline_benchmarks:
                              current_mean = bench['stats']['mean']
                              baseline_mean = baseline_benchmarks[name]['stats']['mean']
                              regression = ((current_mean - baseline_mean) / baseline_mean) * 100

                              if regression > 10:
                                  regressions.append({
                                      'name': name,
                                      'regression': regression,
                                      'baseline': baseline_mean * 1000,
                                      'current': current_mean * 1000
                                  })

                      if regressions:
                          report.write("⚠️ **Performance Regressions Detected:**\n\n")
                          report.write("| Test | Baseline (ms) | Current (ms) | Regression |\n")
                          report.write("|------|---------------|--------------|------------|\n")
                          for reg in regressions:
                              report.write(f"| {reg['name']} | {reg['baseline']:.3f} | {reg['current']:.3f} | +{reg['regression']:.1f}% |\n")

                          # Exit with error if regressions found
                          sys.exit(1)
                      else:
                          report.write("✅ No significant performance regressions detected.\n")
                  else:
                      report.write("\n*No baseline available for comparison. This run will be used as the new baseline.*\n")

          except Exception as e:
              print(f"Error generating report: {e}")
              sys.exit(1)
          EOF

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            benchmark-report.md
            .benchmarks/
          retention-days: 90

      - name: Save as baseline (main branch only)
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-baseline
          path: .benchmarks/
          retention-days: 365

      - name: Generate performance histogram
        if: always()
        run: |
          pytest tests/test_performance.py --benchmark-only --benchmark-histogram=benchmark-histogram

      - name: Upload histogram
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-histogram
          path: benchmark-histogram.svg
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            if (!fs.existsSync('benchmark-report.md')) {
              console.log('No benchmark report found');
              return;
            }

            const report = fs.readFileSync('benchmark-report.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Performance Benchmark Results')
            );

            const commentBody = report + '\n\n---\n*Updated at: ' + new Date().toISOString() + '*';

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: commentBody
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }

      - name: Check for regressions
        run: |
          if [ -f benchmark-report.md ] && grep -q "Performance Regressions Detected" benchmark-report.md; then
            echo "❌ Performance regressions detected!"
            exit 1
          fi
          echo "✅ No performance regressions!"
